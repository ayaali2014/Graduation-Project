{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8812118,"sourceType":"datasetVersion","datasetId":5300630},{"sourceId":8040896,"sourceType":"datasetVersion","datasetId":4740666}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import cv2\nimport os\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\ndef load_frames(directory , image_size=(112, 112)):\n    images = []\n    for root, dirs, files in os.walk(directory):\n      for file in files:\n        image_path = os.path.join(root, file)\n        image = cv2.imread(image_path)\n\n        if image is not None:\n          image = cv2.resize(image, image_size)\n          image = image.astype(np.float32) / 255.0  # Normalize to [0, 1]\n          images.append(image)\n\n    images = np.array(images)\n    return images","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport csv\nimport os\n\ndef file_text(video_path):\n  for root, dirs, files in os.walk(video_path):\n    for file in files:\n      print(file)\n      if file.endswith(\".mp4\"):\n        video_path = os.path.join(root, file)\n        \n  # Open the MP4 file using OpenCV\n  cap = cv2.VideoCapture(video_path)\n\n  frames = []\n\n  # Read frames until the video ends or the end time is reached\n  while cap.isOpened():\n    ret, frame = cap.read()\n\n    if not ret :\n        break\n\n    frames.append(frame)\n  print(len(frames))\n  cap.release()\n\n  path = '/kaggle/working/Frames' \n  if not os.path.exists(path):\n    os.makedirs(path)\n\n  for j, frame in enumerate(frames):\n      final_frame = mouthDetection(frame)\n      if np.all(final_frame == 0):\n        print(\"not valid\")\n        continue\n      cv2.imwrite(path+'/'+'%d.jpg' %j,final_frame)\n      # res = mouthDetection(frame)\n      # norm_res= normalize_frames(res)\n      # word_frames.append(norm_res)","metadata":{"execution":{"iopub.status.busy":"2024-06-28T17:08:49.219992Z","iopub.execute_input":"2024-06-28T17:08:49.220403Z","iopub.status.idle":"2024-06-28T17:08:49.230133Z","shell.execute_reply.started":"2024-06-28T17:08:49.22037Z","shell.execute_reply":"2024-06-28T17:08:49.228886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\ndef mouthDetection(image):\n\n  #from google.colab.patches import cv2_imshow\n\n  # Load the pre-trained Haar Cascade classifier for face detection\n  face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n\n  # Convert the image to grayscale\n  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n  # Perform face detection\n  # minneighbors 16\n  faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n  local_lip = lip_localization(gray)\n  counter = 0\n  mouth_gray = 0\n  # Iterate over the detected faces\n  for (x, y, w, h) in faces:\n      # Define the region of interest (ROI) for the mouth\n      roi_x = x\n      roi_y = y + int(h * 2 / 3)  # Adjust the y-coordinate to focus on the lower part of the face\n      roi_w = w\n      roi_h = int(h / 3)\n\n      # Extract the mouth region from the grayscale image\n      mouth_gray = local_lip[roi_y:roi_y + roi_h, roi_x:roi_x + roi_w]\n\n\n  return mouth_gray","metadata":{"execution":{"iopub.status.busy":"2024-06-28T17:07:48.31818Z","iopub.execute_input":"2024-06-28T17:07:48.318601Z","iopub.status.idle":"2024-06-28T17:07:48.566557Z","shell.execute_reply.started":"2024-06-28T17:07:48.318571Z","shell.execute_reply":"2024-06-28T17:07:48.565387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install dlib\n","metadata":{"execution":{"iopub.status.busy":"2024-06-28T19:53:19.881868Z","iopub.execute_input":"2024-06-28T19:53:19.882635Z","iopub.status.idle":"2024-06-28T20:00:49.771299Z","shell.execute_reply.started":"2024-06-28T19:53:19.882603Z","shell.execute_reply":"2024-06-28T20:00:49.770149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport dlib\n\ndef lip_localization(image):\n  # Load the detector\n  detector = dlib.get_frontal_face_detector()\n  # Load the predictor\n  predictor = dlib.shape_predictor(\"/kaggle/input/face-landmarks/shape_predictor_68_face_landmarks.dat\")\n  # Convert image into grayscale\n  #gray = cv2.cvtColor(src=image, code=cv2.COLOR_BGR2GRAY)\n  # Use detector to find landmarks\n  faces = detector(image)\n  for face in faces:\n      x1 = face.left() # left point\n      y1 = face.top() # top point\n      x2 = face.right() # right point\n      y2 = face.bottom() # bottom point\n      # Create landmark object\n      landmarks = predictor(image=image, box=face)\n      # Loop through all the points\n      for n in range(48, 60):\n          x = landmarks.part(n).x\n          y = landmarks.part(n).y\n          # Draw a circle\n          cv2.circle(img=image, center=(x, y), radius=1, color=(255, 255, 255), thickness=-1)\n  return image","metadata":{"execution":{"iopub.status.busy":"2024-06-28T17:07:43.686986Z","iopub.execute_input":"2024-06-28T17:07:43.687359Z","iopub.status.idle":"2024-06-28T17:07:43.698112Z","shell.execute_reply.started":"2024-06-28T17:07:43.687331Z","shell.execute_reply":"2024-06-28T17:07:43.696403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_text(\"/kaggle/input/gbDataset\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frames = load_frames(\"/kaggle/working/Frames\")\nframes = frames.reshape((-1, 1,112, 112, 3))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport keras\n#from tensorflow_addons.layers import TFSMLayer\n#tf.keras.models.load_weights(\"/content/model.h5\")\n#keras.layers.TFSMLayer(\"model1\", call_endpoint=\"serving_default\")\nreconstructed_model =  tf.keras.models.load_model(\"/kaggle/input/final-ds/model (1).h5\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predict = reconstructed_model.predict(frames)\ny_categorical = np.argmax(y_predict, axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\ndef read_words_to_dict(filename):\n        with open(filename, 'r') as file:\n            word_list = file.read().splitlines()\n        word_dict = {}\n        for line in word_list:\n            index, word = line.split(':')\n            word_dict[index] = word\n\n        return word_dict\n\nword_dict = read_words_to_dict('/kaggle/input/final-ds/class_names.txt')\nprint(word_dict.keys())","metadata":{"execution":{"iopub.status.busy":"2024-06-28T19:45:45.629496Z","iopub.execute_input":"2024-06-28T19:45:45.630313Z","iopub.status.idle":"2024-06-28T19:45:45.639885Z","shell.execute_reply.started":"2024-06-28T19:45:45.630282Z","shell.execute_reply":"2024-06-28T19:45:45.638785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fr = np.median(y_categorical)\ndirectory_path = \"/kaggle/working/\"\n\n# Create the directory if it doesn't exist\nif not os.path.exists(directory_path):\n    os.makedirs(directory_path)\n\n# Construct the full file path\nfile_path = os.path.join(directory_path, \"arabic_word.txt\")\n\n# Open the file in write mode with UTF-8 encoding\nwith open(file_path, \"w\", encoding=\"utf-8\") as file:\n    # Write the Arabic word to the file\n    file.write(word_dict[str(int(fr))])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}